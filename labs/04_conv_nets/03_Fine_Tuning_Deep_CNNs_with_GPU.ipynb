{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fine Tuning a pre-trained Deep CNN on a GPU machine\n\nThis session is inspired by [a blog post](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) by FranÃ§ois Chollet, the creator of the Keras library.\n\n**WARNING**: the execution of notebook **requires a GPU** e.g. with **at least 6GB of GPU RAM**.\n\n**macOS users**: If you have an Apple Silicon Mac (M1/M2/M3/M4), you can use MPS (Metal Performance Shaders) for GPU acceleration. MPS typically provides 2-5x speedup compared to CPU. The code below will automatically detect and use the best available device.\n\nFor this session we are going to use a cats vs dogs image classification dataset.\n\n## Running on Kaggle (recommended for GPU access)\n\nIt is recommended to do this notebook from the [kaggle kernels](https://www.kaggle.com/kernels) hosted interface that provides GPU hours for free:\n\n- login at [kaggle kernels](https://www.kaggle.com/kernels);\n- click the **new notebook** button;\n- upload this notebook file from the \"File\" menu;\n- in the \"File\" menu \"Add or upload data\" and choose to add the Dogs vs. Cats dataset;\n- the data should be available in the `/kaggle/input/dogs-vs-cats` folder of your kaggle kernel session;\n- enable \"Internet\" and \"GPU\" in the \"Settings\" panel of this kernel.\n\n## Running locally\n\nTo download the data locally, install the Kaggle CLI and download the Microsoft Cats vs Dogs dataset:\n\n```bash\npip install kaggle\n\n# Create and configure your API key from https://www.kaggle.com/settings\n# Save it to ~/.kaggle/kaggle.json\n\n# Download the dataset\nmkdir -p ~/data/dogs-vs-cats\ncd ~/data/dogs-vs-cats\nkaggle datasets download -d shaunthesheep/microsoft-catsvsdogs-dataset\nunzip microsoft-catsvsdogs-dataset.zip\n\n# Reorganize into expected structure\nmkdir -p train\nmv PetImages/Cat train/cat\nmv PetImages/Dog train/dog\nrmdir PetImages\n```\n\nOnce this is done we can proceed with loading the data:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ[\"KERAS_BACKEND\"] = \"torch\"\n\nimport torch\n\n# Detect best available device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    device_name = f\"CUDA ({torch.cuda.get_device_name(0)})\"\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    device_name = \"MPS (Apple Metal)\"\nelse:\n    device = torch.device(\"cpu\")\n    device_name = \"CPU\"\n\nprint(f\"Using device: {device_name}\")\nprint(f\"PyTorch version: {torch.__version__}\")\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport keras\n\nimport os.path as op\nimport shutil\nfrom zipfile import ZipFile"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# When working from manually downloaded files (default for local execution):\ndata_folder = op.expanduser('~/data/dogs-vs-cats')\nworking_folder = data_folder\n\n# When running on Kaggle, uncomment these instead:\n# data_folder = '/kaggle/input/dogs-vs-cats'\n# working_folder = \"/kaggle/working\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The train folder should already exist with cat/ and dog/ subfolders\n# If using the original Kaggle competition data, you may need to extract train.zip first:\n# train_zip = op.join(data_folder, 'train.zip')\n# if not op.exists(train_folder) and op.exists(train_zip):\n#     print('Extracting %s...' % train_zip)\n#     ZipFile(train_zip).extractall(working_folder)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_folder = op.join(working_folder, 'train')\n\n# Verify train folder exists\nif op.exists(train_folder):\n    print(f\"Train folder found: {train_folder}\")\n    print(f\"Contents: {os.listdir(train_folder)}\")\nelse:\n    print(f\"WARNING: Train folder not found at {train_folder}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras image data helpers want images for different classes ('cat' and 'dog') to live in distinct subfolders. Let's rearrange the image files to follow that convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_folders(folder):\n",
    "    image_filenames = [op.join(folder, fn) for fn in os.listdir(folder)\n",
    "                       if fn.endswith('.jpg')]\n",
    "    if len(image_filenames) == 0:\n",
    "        return\n",
    "    print(\"Rearranging %d images in %s into one subfolder per class...\"\n",
    "          % (len(image_filenames), folder))\n",
    "    for image_filename in image_filenames:\n",
    "        subfolder, _ = image_filename.split('.', 1)\n",
    "        subfolder = op.join(folder, subfolder)\n",
    "        if not op.exists(subfolder):\n",
    "            os.mkdir(subfolder)\n",
    "        shutil.move(image_filename, subfolder)\n",
    "\n",
    "rearrange_folders(train_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a validation dataset by taking 500 images of cats and 500 images of dogs out of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validation = 500\n",
    "\n",
    "validation_folder = op.join(working_folder, 'validation')\n",
    "if not op.exists(validation_folder):\n",
    "    os.mkdir(validation_folder)\n",
    "    for class_name in ['dog', 'cat']:\n",
    "        train_subfolder = op.join(train_folder, class_name)\n",
    "        validation_subfolder = op.join(validation_folder, class_name)\n",
    "        print(\"Populating %s...\" % validation_subfolder)\n",
    "        os.mkdir(validation_subfolder)\n",
    "        images_filenames = sorted(os.listdir(train_subfolder))\n",
    "        for image_filename in images_filenames[-n_validation:]:\n",
    "            shutil.move(op.join(train_subfolder, image_filename),\n",
    "                        validation_subfolder)\n",
    "        print(\"Moved %d images\" % len(os.listdir(validation_subfolder)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Data Augmentation\n",
    "\n",
    "\n",
    "Let's use keras utilities to manually load the first image file of the cat folder. If keras complains about the missing \"PIL\" library, make sure to install it with one of the following commands:\n",
    "\n",
    "```bash\n",
    "conda install pillow\n",
    "\n",
    "# or\n",
    "\n",
    "pip install pillow\n",
    "```\n",
    "\n",
    "You might need to restart the kernel of this notebook to get Keras work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from keras.utils import array_to_img, img_to_array, load_img\n\n# Load a sample cat image\nsample_images = os.listdir(op.join(train_folder, 'cat'))\nsample_image = sample_images[0]\nimg = load_img(op.join(train_folder, 'cat', sample_image))\nx = img_to_array(img)\n\nprint(f\"Loaded: {sample_image}\")\nprint(x.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x.astype(np.uint8))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides tools to generate many variations from a single image: this is useful to augment the dataset with variants that should not affect the image label: a rotated image of a cat is an image of a cat.\n",
    "\n",
    "Doing data augmentation at train time make neural networks ignore such label-preserving transformations and therefore help reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from keras import layers\n\n# Data augmentation using Keras 3 preprocessing layers\ndata_augmentation = keras.Sequential([\n    layers.RandomRotation(0.1),  # ~40 degrees = 0.1 * 360\n    layers.RandomTranslation(0.2, 0.2),\n    layers.RandomZoom(0.2),\n    layers.RandomFlip(\"horizontal\"),\n], name=\"data_augmentation\")\n\n# For visualization, let's augment a single image\nx_batch = np.expand_dims(x / 255.0, axis=0)  # Add batch dimension and normalize\nplt.figure(figsize=(11, 5))\nfor i in range(15):\n    x_augmented = data_augmentation(x_batch, training=True)\n    plt.subplot(3, 5, i + 1)\n    plt.imshow(x_augmented[0])\n    plt.axis('off')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell removed - augmentation visualization moved to previous cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "In Keras 3, we use `image_dataset_from_directory` to load images and Keras preprocessing layers for augmentation. The preprocessing layers can be integrated directly into the model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load and display augmented images from directory using Keras 3 API\ntrain_ds = keras.utils.image_dataset_from_directory(\n    train_folder,\n    image_size=(224, 224),\n    batch_size=1,\n    label_mode='binary',\n)\n\nplt.figure(figsize=(11, 5))\nfor i, (images, labels) in enumerate(train_ds.take(15)):\n    augmented = data_augmentation(images, training=True)\n    plt.subplot(3, 5, i + 1)\n    plt.imshow(augmented[0])\n    plt.axis('off')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pre-trained computer vision model\n",
    "\n",
    "Let us load a state of the art model with a good tradeoff between prediction speed, model size and predictive accuracy, namely a Residual Network with 54 parameterized layers (53 convolutional + 1 fully connected for the softmax):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from keras.applications.resnet50 import ResNet50, preprocess_input\n\nfull_imagenet_model = ResNet50(weights='imagenet')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(full_imagenet_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have `graphviz` system package and the `pydot_ng` python package installed you can uncomment the following cell to display the structure of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# model_viz = model_to_dot(full_imagenet_model,\n",
    "#                          show_layer_names=False,\n",
    "#                          show_shapes=True)\n",
    "# SVG(model_viz.create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "Let's remove the last dense classification layer that is specific to the image net classes and use the previous layer (after flattening) as a feature extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from keras.models import Model\n\noutput = full_imagenet_model.layers[-2].output\nbase_model = Model(full_imagenet_model.input, output)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using this model we need to be careful to apply the same image processing as was used during the training, otherwise the marginal distribution of the input pixels might not be on the right scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# In Keras 3, preprocess_input handles batched inputs directly\n# We can use it in dataset.map() pipelines"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "batch_size = 50\n\ntrain_ds = keras.utils.image_dataset_from_directory(\n    train_folder,\n    image_size=(224, 224),\n    batch_size=batch_size,\n    label_mode='binary',\n    shuffle=True,\n)\n\n# Apply preprocessing (ResNet expects specific normalization)\ntrain_ds_preprocessed = train_ds.map(\n    lambda x, y: (preprocess_input(x), y)\n)\n\n# Get a sample batch\nfor X, y in train_ds_preprocessed.take(1):\n    print(X.shape, y.shape)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: write a function that iterate of over 5000 images in the training set (bach after batch), extracts the activations of the last layer of `base_model` (by calling predicts) and collect the results in a big numpy array with dimensions `(5000, 2048)` for the features and `(5000,)` for the matching image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/dogs_vs_cats_extract_features.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load precomputed features if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading precomputed features\")\n",
    "labels_train = np.load('labels_train.npy')\n",
    "features_train = np.load('features_train.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a simple linear model on those features. First let's check that the resulting small dataset has balanced classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = features_train.shape\n",
    "print(n_features, \"features extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\n\n\ntop_model = Sequential()\ntop_model.add(Dense(1, input_dim=n_features, activation='sigmoid'))\ntop_model.compile(optimizer=Adam(learning_rate=1e-4),\n                  loss='binary_crossentropy', metrics=['accuracy'])\n\ntop_model.fit(features_train, labels_train,\n              validation_split=0.1, verbose=2, epochs=15)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright so the transfer learning is already at ~0.98 / 0.99 accuracy. This is not too surprising as the cats and dogs classes are already part of the imagenet label set.\n",
    "\n",
    "Note that this is **already as good or slightly better than the winner of the original kaggle competition** [three years ago](https://www.kaggle.com/c/dogs-vs-cats/leaderboard). At that time they did not have pretrained resnet models at hand.\n",
    "\n",
    "Or validation set has 1000 images, so an accuracy of 0.990 means only 10 classification errors.\n",
    "\n",
    "Let's plug this on top the base model to be able to use it to make some classifications on our held out validation image folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(base_model.input, top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "val_ds = keras.utils.image_dataset_from_directory(\n    validation_folder, \n    image_size=(224, 224),\n    batch_size=1,\n    label_mode='binary',\n    shuffle=False,\n)\n\nplt.figure(figsize=(12, 8))\nfor i, (X, y) in enumerate(val_ds.take(15)):\n    plt.subplot(3, 5, i + 1)\n    plt.imshow(X[0].numpy().astype('uint8'))\n    X_preprocessed = preprocess_input(X.numpy())\n    prediction = model.predict(X_preprocessed, verbose=0)\n    label = \"dog\" if y[0] > 0.5 else \"cat\"\n    plt.title(\"dog prob=%0.4f\\ntrue label: %s\"\n              % (prediction[0][0], label))\n    plt.axis('off')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the validation score on the full validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "val_ds_batched = keras.utils.image_dataset_from_directory(\n    validation_folder,\n    image_size=(224, 224), \n    batch_size=batch_size,\n    label_mode='binary',\n    shuffle=False,\n)\n\n# Apply preprocessing\nval_ds_preprocessed = val_ds_batched.map(\n    lambda x, y: (preprocess_input(x), y)\n)\n\nall_correct = []\nfor X, y in val_ds_preprocessed:\n    predictions = model.predict(X, verbose=0).ravel()\n    y_numpy = y.numpy().ravel()\n    correct = list((predictions > 0.5) == y_numpy)\n    all_correct.extend(correct)\n    print(\"Processed %d images\" % len(all_correct))\n    \nprint(\"Validation accuracy: %0.4f\" % np.mean(all_correct))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** display the example where the model makes the most confident mistakes.\n",
    "\n",
    "To display images in jupyter notebook you can use:\n",
    "\n",
    "```python\n",
    "from IPython.display import Image, display\n",
    "import os.path as op\n",
    "\n",
    "display(Image(op.join(validation_folder, image_name)))\n",
    "```\n",
    "\n",
    "The filenames of items sampled by a flow (without random shuffling) can be accessed via: `val_flow.filenames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/dogs_vs_cats_worst_predictions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning\n",
    "\n",
    "Let's identify the location of the residual blocks (merge by addition in a residual architecture):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": "from keras.layers import Add\n\n[(i, l.output_shape)\n for (i, l) in enumerate(model.layers)\n if isinstance(l, Add)]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the weights of the low level layers and fine tune the top level layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    layer.trainable = i >= 151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fine tune a bit the top level layers to see if we can further improve the accuracy. Use the **nvidia-smi** command in a bash terminal on the server to monitor the GPU usage when the model is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from keras import optimizers\n\n# Data augmentation layers for fine-tuning\nfine_tune_augmentation = keras.Sequential([\n    layers.RandomRotation(0.05),  # ~20 degrees\n    layers.RandomTranslation(0.2, 0.2),\n    layers.RandomZoom(0.2),\n    layers.RandomFlip(\"horizontal\"),\n], name=\"fine_tune_augmentation\")\n\n# Create training dataset with augmentation and preprocessing\ntrain_ds_finetune = keras.utils.image_dataset_from_directory(\n    train_folder,\n    image_size=(224, 224),\n    batch_size=batch_size,\n    label_mode='binary',\n    shuffle=True,\n    seed=0,\n)\n\n# Apply augmentation then preprocessing\ndef augment_and_preprocess(x, y):\n    x = fine_tune_augmentation(x, training=True)\n    x = preprocess_input(x)\n    return x, y\n\ntrain_ds_augmented = train_ds_finetune.map(augment_and_preprocess)\n\n# Validation dataset (no augmentation)\nval_ds_finetune = keras.utils.image_dataset_from_directory(\n    validation_folder,\n    image_size=(224, 224),\n    batch_size=batch_size,\n    label_mode='binary',\n    shuffle=False,\n)\nval_ds_finetune = val_ds_finetune.map(lambda x, y: (preprocess_input(x), y))\n\nopt = optimizers.SGD(learning_rate=1e-4, momentum=0.9)\nmodel.compile(optimizer=opt, loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Compute steps per epoch\nsteps_per_epoch = 5000 // batch_size\nvalidation_steps = 1000 // batch_size  # validation set has ~1000 images\n\nhistory = model.fit(\n    train_ds_augmented,\n    steps_per_epoch=steps_per_epoch,\n    epochs=30,\n    validation_data=val_ds_finetune,\n    validation_steps=validation_steps,\n)\n\n# Note: the pretrained model was already very good. Fine tuning\n# does not really seem to help. It might be more interesting to\n# introspect the quality of the labeling in the training set to\n# check for images that are too ambiguous and should be removed\n# from the training set."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus exercise**: train your own architecture from scratch using adam and data augmentation. Start with a small architecture first (e.g. 4 convolutions layers interleaved with 2 max pooling layers followed by a `Flatten` and two fully connected layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}