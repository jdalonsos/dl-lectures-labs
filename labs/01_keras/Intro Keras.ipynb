{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks with Keras\n",
    "\n",
    "### Goals: \n",
    "- Intro: train a neural network with Keras 3 (PyTorch backend)\n",
    "\n",
    "### Dataset:\n",
    "- Digits: 10 class handwritten digits\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Keras backend to PyTorch (must be done before importing keras)\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split\n",
    "\n",
    "Let's keep some held-out data to be able to measure the generalization performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the Input Data\n",
    "\n",
    "\n",
    "Make sure that all input variables are approximately on the same scale via input normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(scaler.mean_)\n",
    "# print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the one of the transformed sample (after feature standardization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_train[sample_index].reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"transformed sample\\n(standardization)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler objects makes it possible to recover the original sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(scaler.inverse_transform(X_train[sample_index:sample_index+1]).reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"original sample\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the Target Data\n",
    "\n",
    "\n",
    "To train a first neural network we also need to turn the target variable into a vector \"one-hot-encoding\" representation. Here are the labels of the first samples in the training set encoded as integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a utility function to convert integer-encoded categorical variables as one-hot encoded values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Networks with Keras\n",
    "\n",
    "Objectives of this section:\n",
    "\n",
    "- Build and train a first feedforward network using `Keras`\n",
    "    - https://keras.io/guides/\n",
    "- Experiment with different optimizers, activations, size of layers, initializations\n",
    "\n",
    "### A First Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build an train a our first feed forward neural network using the high level API from keras:\n",
    "\n",
    "- first we define the model by stacking layers with the right dimensions\n",
    "- then we define a loss function and plug the SGD optimizer\n",
    "- then we feed the model the training data for fixed number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 100\n",
    "output_dim = Y_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_dim, input_dim=input_dim, activation=\"tanh\"))\n",
    "model.add(Dense(output_dim, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=0.1),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, validation_split=0.2, epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Convergence\n",
    "\n",
    "Let's wrap the keras history info into a pandas dataframe for easier plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df[\"epoch\"] = history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(12, 6))\n",
    "history_df.plot(x=\"epoch\", y=[\"loss\", \"val_loss\"], ax=ax0)\n",
    "history_df.plot(x=\"epoch\", y=[\"accuracy\", \"val_accuracy\"], ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Convergence with Tensorboard\n",
    "\n",
    "**Note:** The Keras TensorBoard callback requires TensorFlow. Since we're using the PyTorch backend, this section is skipped. You can use PyTorch's native `torch.utils.tensorboard.SummaryWriter` for tensorboard logging instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipped - requires TensorFlow\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipped - requires TensorFlow\n",
    "# !rm -rf tensorboard_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipped - Keras TensorBoard callback requires TensorFlow\n",
    "# To use tensorboard with PyTorch backend, use torch.utils.tensorboard.SummaryWriter\n",
    "\n",
    "# import datetime\n",
    "# from keras.callbacks import TensorBoard\n",
    "# \n",
    "# model = Sequential()\n",
    "# model.add(Dense(hidden_dim, input_dim=input_dim, activation=\"tanh\"))\n",
    "# model.add(Dense(output_dim, activation=\"softmax\"))\n",
    "# \n",
    "# model.compile(optimizer=optimizers.SGD(learning_rate=0.1),\n",
    "#               loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# \n",
    "# timestamp =  datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# log_dir = \"tensorboard_logs/\" + timestamp\n",
    "# tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# \n",
    "# model.fit(x=X_train, y=Y_train, validation_split=0.2, epochs=15,\n",
    "#           callbacks=[tensorboard_callback]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipped - requires TensorFlow\n",
    "# %tensorboard --logdir tensorboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Exercises: Impact of the Optimizer\n",
    "\n",
    "- Try to decrease the learning rate value by 10 or 100. What do you observe?\n",
    "\n",
    "- Try to increase the learning rate value to make the optimization diverge.\n",
    "\n",
    "- Configure the SGD optimizer to enable a Nesterov momentum of 0.9\n",
    "  \n",
    "**Notes**: \n",
    "\n",
    "The keras API documentation is available at:\n",
    "\n",
    "https://keras.io/api/\n",
    "\n",
    "It is also possible to learn more about the parameters of a class by using the question mark: type and evaluate:\n",
    "\n",
    "```python\n",
    "optimizers.SGD?\n",
    "```\n",
    "\n",
    "in a jupyter notebook cell.\n",
    "\n",
    "It is also possible to type the beginning of a function call / constructor and type \"shift-tab\" after the opening paren:\n",
    "\n",
    "```python\n",
    "optimizers.SGD(<shiff-tab>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers.SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_sgd_and_momentum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace the SGD optimizer by the Adam optimizer from keras and run it\n",
    "  with the default parameters.\n",
    "\n",
    "  Hint: use `optimizers.<TAB>` to tab-complete the list of implemented optimizers in Keras.\n",
    "\n",
    "- Add another hidden layer and use the \"Rectified Linear Unit\" for each\n",
    "  hidden layer. Can you still train the model with Adam with its default global\n",
    "  learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_adam.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises: Forward Pass and Generalization\n",
    "\n",
    "- Compute predictions on test set using `model.predict(...)`\n",
    "- Compute average accuracy of the model on the test set: the fraction of test samples for which the model makes a prediction that matches the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_accuracy_on_test_set.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us decompose how we got the predictions. First, we call the model on the data to get the last layer (softmax) outputs directly as a PyTorch Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(X_test)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions), predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that for each row, the probabilities sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.sum(axis=1)[:5].detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract the label with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(predictions.detach().numpy(), axis=1)\n",
    "predicted_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare those labels to the expected labels to compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (predicted_labels == y_test).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: with Keras 3 and PyTorch backend, you can convert tensors to numpy arrays using `.detach().numpy()` (detach is needed for tensors that require gradients):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_np = predictions.detach().numpy()\n",
    "predictions_np[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predicted_labels == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Assignment: Impact of Initialization\n",
    "\n",
    "Let us now study the impact of a bad initialization when training\n",
    "a deep feed forward network.\n",
    "\n",
    "By default Keras dense layers use the \"Glorot Uniform\" initialization\n",
    "strategy to initialize the weight matrices:\n",
    "\n",
    "- each weight coefficient is randomly sampled from [-scale, scale]\n",
    "- scale is proportional to $\\frac{1}{\\sqrt{n_{in} + n_{out}}}$\n",
    "\n",
    "This strategy is known to work well to initialize deep neural networks\n",
    "with \"tanh\" or \"relu\" activation functions and then trained with\n",
    "standard SGD.\n",
    "\n",
    "To assess the impact of initialization let us plug an alternative init\n",
    "scheme into a 2 hidden layers networks with \"tanh\" activations.\n",
    "For the sake of the example let's use normal distributed weights\n",
    "with a manually adjustable scale (standard deviation) and see the\n",
    "impact the scale value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "\n",
    "normal_init = initializers.TruncatedNormal(stddev=0.01)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_dim, input_dim=input_dim, activation=\"tanh\",\n",
    "                kernel_initializer=normal_init))\n",
    "model.add(Dense(hidden_dim, activation=\"tanh\",\n",
    "                kernel_initializer=normal_init))\n",
    "model.add(Dense(output_dim, activation=\"softmax\",\n",
    "                kernel_initializer=normal_init))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=0.1),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the parameters of the first layer after initialization but before any training has happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.layers[0].weights[0].numpy()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model.layers[0].weights[1].numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=15, batch_size=32)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['loss'], label=\"Truncated Normal init\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been fit, the weights have been updated and notably the biases are no longer 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "- Try the following initialization schemes and see whether\n",
    "  the SGD algorithm can successfully train the network or\n",
    "  not:\n",
    "  \n",
    "  - a very small e.g. `stddev=1e-3`\n",
    "  - a larger scale e.g. `stddev=1` or `10`\n",
    "  - initialize all weights to 0 (constant initialization)\n",
    "  \n",
    "- What do you observe? Can you find an explanation for those\n",
    "  outcomes?\n",
    "\n",
    "- Are more advanced solvers such as SGD with momentum or Adam able\n",
    "  to deal better with such bad initializations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_initializations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_initializations_analysis.py"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "dlclass2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
